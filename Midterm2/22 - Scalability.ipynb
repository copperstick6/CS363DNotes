{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Market basket data can be huge, have to consider DB I/O cost, merging work across distributed systems and memory size limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing data is not trivial, can be stored in the following:   \n",
    "2D array adjacency matrix, hashmap method, 1D triangular method (row by row storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most memory is required for determining frequent pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCY algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first pass, there is memory to spare, so keep one item set items in an array and keep counts, use this array for constructing 2-item sets and hash them to a bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a sample dataset that will work with the available memory size, must be truly random, can produce false negatives and positives, but we can eliminate false positives and mitigate false negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SON algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improves upon sampling, divide data set into chunks or however large your memory can handle, process each chunk as a sample, then take the union of all frequent itemsets of all chunks, compare against full data set to get frequent itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New transactions are constantly being added, stremas evolve over time, frequent itemsets change, minsup must be considered as a percentage.    \n",
    "Sampling method:  \n",
    "1. Collect some amount of transactions, find frequent itemsets here\n",
    "2. Keep collecting streaming data in the background\n",
    "3. Find frequent itemsets in sample 2, update support percentages, if a frequent itemset drops below the threshold, remove from list\n",
    "4. Mine sample 2 for new itemsets, add to frequent itemsets\n",
    "5. repeat for new sample collected while mining sample 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
